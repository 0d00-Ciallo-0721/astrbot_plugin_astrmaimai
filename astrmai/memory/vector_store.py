import os
import json
import time
import faiss
import numpy as np
from typing import List, Dict, Any
from astrbot.api import logger
from astrbot.core.db.vec_db.faiss_impl.vec_db import FaissVecDB
from .utils import SearchResult, TextProcessor

# ğŸ‘‡ è¡¥å……ç¼ºå¤±çš„ EmbeddingClient å¯¼å…¥
from .embedding import EmbeddingClient

class VectorRetriever:
    """
    å‘é‡æ£€ç´¢å™¨ (Custom Faiss implementation)
    """
    def __init__(self, data_path: str, embedding_client: EmbeddingClient):
        self.data_path = data_path
        self.embedding_client = embedding_client
        self.processor = TextProcessor()
        
        # ç´¢å¼•ä¸å…ƒæ•°æ®å­˜å‚¨è·¯å¾„
        self.index_path = os.path.join(data_path, "faiss_index.bin")
        self.meta_path = os.path.join(data_path, "faiss_meta.json")
        
        self.index = None
        self.metadata_store = {}
        self.next_id = 0
        
        self._load()

    def _load(self):
        """åŠ è½½æœ¬åœ°ç´¢å¼•ä¸å…ƒæ•°æ®"""
        if os.path.exists(self.meta_path):
            try:
                with open(self.meta_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.metadata_store = {int(k): v for k, v in data.items()}
                    self.next_id = max(self.metadata_store.keys()) + 1 if self.metadata_store else 0
            except Exception as e:
                logger.error(f"[VectorStore] å…ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")

        if os.path.exists(self.index_path):
            try:
                self.index = faiss.read_index(self.index_path)
            except Exception as e:
                logger.error(f"[VectorStore] Faiss ç´¢å¼•åŠ è½½å¤±è´¥: {e}")

    def _save(self):
        """æŒä¹…åŒ–ä¿å­˜"""
        if self.index is not None:
            faiss.write_index(self.index, self.index_path)
        with open(self.meta_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata_store, f, ensure_ascii=False)

    async def add_document(self, content: str, metadata: Dict[str, Any]) -> int:
        """å­˜å…¥æ–‡æœ¬ï¼Œè¿”å› document id"""
        if "create_time" not in metadata:
            metadata["create_time"] = time.time()
            
        # 1. æ˜¾å¼è°ƒç”¨æ ‡å‡†åŒ– Embedding å®¢æˆ·ç«¯è·å–å‘é‡
        vector = await self.embedding_client.get_vector(content)
        if not vector:
            logger.error("[VectorStore] æ— æ³•è·å–æ–‡æœ¬å‘é‡ï¼Œè·³è¿‡å­˜å…¥")
            return -1
            
        # 2. è½¬æ¢ä¸º Numpy æ•°ç»„å¹¶åš L2 å½’ä¸€åŒ– (ä¸ºåç»­è®¡ç®— Cosine Similarity åšå‡†å¤‡)
        vec_np = np.array([vector], dtype=np.float32)
        faiss.normalize_L2(vec_np)
        
        # 3. åŠ¨æ€åˆå§‹åŒ– Index (è§£å†³ä¸åŒæ¨¡å‹ç»´åº¦ä¸åŒçš„ç—›ç‚¹)
        if self.index is None:
            dim = vec_np.shape[1]
            self.index = faiss.IndexFlatIP(dim) # å†…ç§¯ (Inner Product) åœ¨å½’ä¸€åŒ–åç­‰åŒäºä½™å¼¦ç›¸ä¼¼åº¦
            logger.info(f"[VectorStore] å·²æ ¹æ®æä¾›å•†åŠ¨æ€åˆå§‹åŒ– Faiss ç´¢å¼•ï¼Œç»´åº¦: {dim}")
            
        # 4. æ’å…¥ä¸ä¿å­˜
        doc_id = self.next_id
        self.index.add(vec_np)
        
        self.metadata_store[doc_id] = {
            "content": content,
            "metadata": metadata
        }
        self.next_id += 1
        self._save()
        
        return doc_id


    async def search(self, query: str, k: int = 20) -> List[SearchResult]:
        """æ£€ç´¢æ–‡æœ¬ç‰‡æ®µ"""
        if self.index is None or self.index.ntotal == 0:
            return []
            
        # é¢„å¤„ç†æŸ¥è¯¢
        tokens = self.processor.tokenize(query)
        processed_query = " ".join(tokens) if tokens else query
        
        # è·å–æŸ¥è¯¢å‘é‡
        vector = await self.embedding_client.get_vector(processed_query)
        if not vector:
            return []
            
        vec_np = np.array([vector], dtype=np.float32)
        faiss.normalize_L2(vec_np)
        
        # æ£€ç´¢ (é™åˆ¶æœç´¢èŒƒå›´é˜²æ­¢è¶…é™)
        fetch_k = min(k * 2, self.index.ntotal)
        distances, indices = self.index.search(vec_np, fetch_k)
        
        out = []
        for i, doc_id in enumerate(indices[0]):
            doc_id = int(doc_id) # faiss è¿”å›çš„æ˜¯ int64
            if doc_id == -1 or doc_id not in self.metadata_store:
                continue
            
            score = float(distances[0][i]) # IndexFlatIP å½’ä¸€åŒ–åæ­¤å¤„åˆ†æ•°ä»‹äº -1 åˆ° 1 ä¹‹é—´ (ä½™å¼¦ç›¸ä¼¼åº¦)
            data = self.metadata_store[doc_id]
            
            out.append(SearchResult(
                doc_id=doc_id,
                score=score,
                content=data["content"],
                metadata=data["metadata"],
                source="vector"
            ))
            
        return out