### üìÑ services/memory/text_processor.py
import re
import string
from pathlib import Path
try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

class TextProcessor:
    """ÊñáÊú¨Â§ÑÁêÜÂô®ÔºöÂàÜËØç‰∏éÊ∏ÖÊ¥ó"""
    DEFAULT_STOPWORDS = {
        "Êàë", "‰Ω†", "‰ªñ", "Â•π", "ÂÆÉ", "Êàë‰ª¨", "‰Ω†‰ª¨", "‰ªñ‰ª¨", "ÁöÑ", "‰∫Ü", "ÁùÄ", "ÊòØ", "Âú®"
    }

    def __init__(self, stopwords_path: str = None):
        self.stopwords = self.DEFAULT_STOPWORDS.copy()
        if stopwords_path:
            self._load_stopwords(stopwords_path)

    def _load_stopwords(self, path: str):
        try:
            p = Path(path)
            if p.exists():
                with open(p, 'r', encoding='utf-8') as f:
                    for line in f:
                        word = line.strip()
                        if word: self.stopwords.add(word)
        except Exception:
            pass

    def tokenize(self, text: str, remove_stopwords: bool = True) -> list:
        if not text: return []
        # Ê∏ÖÊ¥ó
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        
        if JIEBA_AVAILABLE:
            tokens = jieba.lcut(text)
        else:
            tokens = text.split() # ÈôçÁ∫ßÂ§ÑÁêÜ

        if remove_stopwords:
            tokens = [t for t in tokens if t.strip() and t not in self.stopwords]
        
        return tokens